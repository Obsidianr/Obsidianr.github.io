<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">

    <title>Title</title>
</head>
<body>
<p style="font-size: 45px">Sdfsdf</p>
<p style="font-size: 18px">
    We present a multimodal dataset for the analysis of human affective states.
    The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos.
    Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance and familiarity.
    For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection was used,
    utilising retrieval by affective tags from the last.fm website, video highlight detection and an online assessment tool.
    <br>
    The dataset is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.
    The dataset was first presented in the following paper:
</p>
</body>
</html>